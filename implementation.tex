\section{Implementation}

\subsection{TensorFlow 101}
TensorFlow~\cite{TensorFlow} is an open-source software library for machine learning developed by the Google Brain Team.
The library models the computations in machine learning as data flow graphs.
Multidimensional data arrays are called tensors in TensorFlow.
Nodes in the graph represent mathematical operations between tensors,
such as add, multiply, softmax and dropout.
Graph edges represent the flow of tensors between nodes.
The computation graph based architecture allow researchers to run or train neural networks
on one or more CPUs or GPUs with unified API.

For general classification task, input $X$ and label $Y$ are defined as \textbf{placeholder}s
and feed into the computation graph at running time using a dictionary.
The graph of a typical deep learning model have three parts.
The \textbf{inference} graph should be built so that output predictions could be returned as tensor.
For example, in the multilayer perceptron case, inference graph contains all iterative
computation~\ref{Equ:MLPFeedForward1}-\ref{Equ:MLPFeedForward2} in the feed-forwarding steps.
The \textbf{loss} graph should compute the loss function defined by particular models.
Usually it is either cross-entropy or mean-squared error averaged across the batch data.
The loss graph will be optimized, usually minimized, by the \textbf{train} part.
This optimization can be conducted by various optimizing algorithms, such as gradient descent,
Momentum, RMSProp.
After sufficient steps of batch training, we can evaluate the trained model with inference
graph and compare the predictions with the test dataset labels.
TensorFlow also provides various useful utilities for training models and running experiments.
Using a Saver, we are able to checkpoint the training process used to later restore a model for
further training or evaluation.
User can also use Summary nodes to log the snapshot of interest variables, which can be
automatically visualized by TensorBoard.

\subsection{NetLearner}
We provide a Python library NetLearner that wraps up several deep learning models on the basis of TensorFlow.
NetLearner modularizes multilayer perceptron, restricted Boltzmann machine, sparse autoencoder
and masking-noise autoencoder, all of which are used to perform the 5-class classification
on the NSL-KDD dataset.

For the multiple layer perceptron, we tried a 4-hidden-layer network with very wide size
in each layer.
The accuracy on training set is very exiting, usually more than 96\%.
However, its performance on test dataset is not satisifacotry at all.
Instead we found out that a single hidden layer with only 16 neurons has terrefic accuracy.
It is trained with stochastic gradient descent (SGD), without regularization, but with dropout of keep
probability 0.8.
We denote this approach as MLP and show its detailed results in the later section.

We build a RBM with 200-hidden units to perform unsupervised feature learning first on the dataset.
The learned features are then fed into a simple softmax regression classifier.
This hybrid model is call RBM in the later section.
We trained the RBM with contrastive divergen of chain length 1, batch size 10 for 100 epochs.
The learning rate at each step is 0.01.
We denote this combination of RBM and softmax regression as RBM in the later section.

We also implemented the self-taught learning architecture proposed in~\cite{STL-NIDS, SparseAE}.
We adopt sparse autoencoder as the unsuperviesed feature learner;
the learnt features will then be classified by a Softmax regression classifier.
We contact the author of~\cite{STL-NIDS} so that we can reproduce their implementation
with the hyperparameters.
For example, the hidden layer size of the sparse autoencoder is 64; the sparsity value $\rho$ is 0.25.
Different from~\cite{STL-NIDS}, we found that using regularization in
neither autoencoder nor softmax regression is helpful.
So we didn't include regularization term in the both autoencoder and softmax cost function.
The autoencoder is trained with batch size 5000 for 100 epochs.
We denote this approach as SAE and report its performance in the later section.

As a variation to the sparse autoencoder based self-taught learning arhictecture,
we explore what will the performance be if we replace sparse autoencoder with denosing autoencoder.
We simple use dropout to emulate the masking noise to build masking noise autoencoder,
in which input is randomly masked out with probability of 0.4.
The size of the autoencoder is xx.
The autoencoder is trained with SGD of batch size 5000 for 100 epochs.
The result of this approach is labeled as DAE in the later section.

One thing to notice is that we use the same seed to randomly initialized the weights and biases
of the softmax regression classifier such that the learnt features from RBM, sparse autoencoder and
denoising autoencoder are comparable.
For the same reason, all the softmax regression classifiers used by RBM, sparse autoencoder and
denoising autoencoder are trained with SDG of batch size xxx for xxx epochs, learning rate 0.01,
with dropout technique of keep probability set to 0.8.

